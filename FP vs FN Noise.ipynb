{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np,os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, power_transform, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import matthews_corrcoef, precision_recall_curve, auc, accuracy_score, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, TomekLinks\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "import seaborn as sns,matplotlib.pyplot as plt\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "from library.configs import CLFS, IMBS, CV, SCORERS\n",
    "from library.utils import evaluate, read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASETS = ['groovy-1_5_7.csv','jruby-1.4.0.csv','lucene-2.9.0.csv','jruby-1.7.0.preview1.csv','groovy-1_6_BETA_1.csv',\n",
    "        'derby-10.2.1.6.csv','wicket-1.5.3.csv','camel-2.9.0.csv','camel-1.4.0.csv','activemq-5.8.0.csv']\n",
    "DATASETS = [f for f in os.listdir(\"JIRA/\") if 'csv' in f]\n",
    "len(DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_TYPES = ['No','FN-Only','FP-Only','Both']\n",
    "CV = RepeatedStratifiedKFold(n_splits=5,n_repeats=2,random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys([('smote', 'dt'), ('smote', 'lr'), ('smote', 'nb'), ('smote', 'svm'), ('smote', 'knn'), ('smote', 'rf'), ('rus', 'dt'), ('rus', 'lr'), ('rus', 'nb'), ('rus', 'svm'), ('rus', 'knn'), ('rus', 'rf'), ('wilson', 'dt'), ('wilson', 'lr'), ('wilson', 'nb'), ('wilson', 'svm'), ('wilson', 'knn'), ('wilson', 'rf'), ('tomek', 'dt'), ('tomek', 'lr'), ('tomek', 'nb'), ('tomek', 'svm'), ('tomek', 'knn'), ('tomek', 'rf'), ('None', 'dt'), ('None', 'lr'), ('None', 'nb'), ('None', 'svm'), ('None', 'knn'), ('None', 'rf')]),\n",
       " 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {}\n",
    "for im,samp in IMBS.items():\n",
    "    for c,clf in CLFS.items():\n",
    "        models[(im,c)] = Pipeline([('samp',samp),('clf',clf)])\n",
    "models.keys(),len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "FRACS = [0,.25,.5,.75,1.0]\n",
    "cols = pd.MultiIndex.from_product([FRACS,IMBS.keys(),CLFS.keys(),[f.__name__ for f in SCORERS]],names=['frac','imb','clf','metric'])\n",
    "df = pd.DataFrame(index=DATASETS,columns=cols)\n",
    "for it,d in enumerate(DATASETS):\n",
    "    print(it)\n",
    "    X,y_noisy,y_real = read_data(d,stats=True)\n",
    "    C = np.argwhere(y_real==1).ravel()\n",
    "    N = np.argwhere(y_noisy==0).ravel()  #PN\n",
    "    idx = np.intersect1d(N,C)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_real,y_noisy).ravel()\n",
    "    print(\"idx:\",len(idx),fn,len(N),len(C))\n",
    "    for frac in FRACS:\n",
    "        print(\"frac:\",frac)\n",
    "        y = y_real.copy()\n",
    "        size = int(frac*len(idx))\n",
    "        to_pollute = np.random.choice(idx.copy(),size=size,replace=False)\n",
    "        y[to_pollute] = 0\n",
    "        print(len(idx),len(to_pollute))\n",
    "        print(f\"{precision_score(y_real,y):.3f},{recall_score(y_real,y):.3f}\",(y_real!=y).sum())\n",
    "        \n",
    "        for k in models:\n",
    "            r = evaluate(models[k],X,y,y_real,CV,SCORERS)\n",
    "            for f in r:\n",
    "                df.loc[d,(frac,k[0],k[1],f)] = r[f].mean()\n",
    "    df.to_csv(\"PN10.csv\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "FRACS = [0,.25,.5,.75,1.0]\n",
    "cols = pd.MultiIndex.from_product([FRACS,IMBS.keys(),CLFS.keys(),[f.__name__ for f in SCORERS]],names=['frac','imb','clf','metric'])\n",
    "df = pd.DataFrame(index=DATASETS,columns=cols)\n",
    "for it,d in enumerate(DATASETS):\n",
    "    X,y_noisy,y_real = read_data(d,stats=True)\n",
    "    C = np.argwhere(y_real==0).ravel()\n",
    "    N = np.argwhere(y_noisy==1).ravel()  #NP\n",
    "    idx = np.intersect1d(N,C)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_real,y_noisy).ravel()\n",
    "    print(\"idx:\",len(idx),fn,len(N),len(C))\n",
    "    for frac in FRACS:\n",
    "        print(\"frac:\",frac)\n",
    "        y = y_real.copy()\n",
    "        size = int(frac*len(idx))\n",
    "        to_pollute = np.random.choice(idx.copy(),size=size,replace=False)\n",
    "        y[to_pollute] = 1\n",
    "        print(len(idx),len(to_pollute))\n",
    "        print(f\"{precision_score(y_real,y):.3f},{recall_score(y_real,y):.3f}\",(y_real!=y).sum())\n",
    "        \n",
    "        for k in models:\n",
    "            r = evaluate(models[k],X,y,y_real,CV,SCORERS)\n",
    "            for f in r:\n",
    "                df.loc[d,(frac,k[0],k[1],f)] = r[f].mean()\n",
    "    df.to_csv(\"NP10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is Under-sampling Bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class FilterPN(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,est):\n",
    "        self.est = est\n",
    "    def fit(self,X,y_noisy,y_real):\n",
    "        C = np.argwhere(y_real==1).ravel()\n",
    "        N = np.argwhere(y_noisy==0).ravel()  #PN\n",
    "        idx = np.intersect1d(N,C)\n",
    "        remaining = np.delete(list(range(len(X))),idx)\n",
    "        X,y = X[remaining], y_noisy[remaining]\n",
    "        rus = RandomUnderSampler();rus.fit_resample(X,y)\n",
    "        X,y = X[rus.sample_indices_],y[rus.sample_indices_]\n",
    "        self.est.fit(X,y)\n",
    "        return self\n",
    "    def predict(self,X):\n",
    "        return self.est.predict(X)\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        return self.est.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Skipping: 0\n",
      "1\n",
      "Skipping: 1\n",
      "2\n",
      "Skipping: 2\n",
      "3\n",
      "Skipping: 3\n",
      "4\n",
      "Skipping: 4\n",
      "5\n",
      "Skipping: 5\n",
      "6\n",
      "Skipping: 6\n",
      "7\n",
      "Skipping: 7\n",
      "8\n",
      "Skipping: 8\n",
      "9\n",
      "Skipping: 9\n",
      "10\n",
      "Skipping: 10\n",
      "11\n",
      "Skipping: 11\n",
      "12\n",
      "Skipping: 12\n",
      "13\n",
      "Skipping: 13\n",
      "14\n",
      "Skipping: 14\n",
      "15\n",
      "Skipping: 15\n",
      "16\n",
      "Skipping: 16\n",
      "17\n",
      "Skipping: 17\n",
      "18\n",
      "Skipping: 18\n",
      "19\n",
      "hive-0.12.0.csv noise:0.087, imb:56.870,46,2616, Shape:(2662, 65)\n",
      "\n",
      "20\n",
      "hive-0.9.0.csv noise:0.179, imb:25.717,53,1363, Shape:(1416, 65)\n",
      "\n",
      "21\n",
      "jruby-1.1.csv noise:0.175, imb:3.540,161,570, Shape:(731, 65)\n",
      "\n",
      "22\n",
      "jruby-1.4.0.csv noise:0.190, imb:3.890,200,778, Shape:(978, 65)\n",
      "\n",
      "23\n",
      "jruby-1.5.0.csv noise:0.218, imb:3.098,276,855, Shape:(1131, 65)\n",
      "\n",
      "24\n",
      "jruby-1.7.0.preview1.csv noise:0.099, imb:8.902,163,1451, Shape:(1614, 65)\n",
      "\n",
      "25\n",
      "lucene-2.3.0.csv noise:0.204, imb:4.031,160,645, Shape:(805, 65)\n",
      "\n",
      "26\n",
      "lucene-2.9.0.csv noise:0.226, imb:3.921,278,1090, Shape:(1368, 65)\n",
      "\n",
      "27\n",
      "lucene-3.0.0.csv noise:0.185, imb:6.037,190,1147, Shape:(1337, 65)\n",
      "\n",
      "28\n",
      "lucene-3.1.csv noise:0.120, imb:7.477,331,2475, Shape:(2806, 65)\n",
      "\n",
      "29\n",
      "wicket-1.3.0-beta2.csv noise:0.184, imb:4.780,305,1458, Shape:(1763, 65)\n",
      "\n",
      "30\n",
      "wicket-1.3.0-incubating-beta-1.csv noise:0.164, imb:4.806,288,1384, Shape:(1672, 65)\n",
      "\n",
      "31\n",
      "wicket-1.5.3.csv noise:0.064, imb:26.720,93,2485, Shape:(2578, 65)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "cols= pd.MultiIndex.from_product([IMBS.keys(),CLFS.keys(),[f.__name__ for f in SCORERS]],names=['imb','clf','metric'])\n",
    "df = pd.DataFrame(index=DATASETS,columns=cols)\n",
    "df = pd.read_csv(\"CorrectNeg.csv\", header=[0,1,2],index_col=0)\n",
    "for it,d in enumerate(DATASETS):\n",
    "    print(it)\n",
    "    if df.loc[d].isna().sum()==0:\n",
    "        print(\"Skipping:\",it)\n",
    "        continue\n",
    "    X,y_noisy,y_real = read_data(d,stats=True)\n",
    "    for k in models:\n",
    "        clf = FilterPN(models[k])\n",
    "        r = evaluate(clf,X,y_noisy,y_real,CV,SCORERS,sample_weight=y_real)\n",
    "        for f in r:\n",
    "            df.loc[d,(k[0],k[1],f)] = r[f].mean()\n",
    "    df.to_csv(\"CorrectNeg.csv\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "1. Both `pr_rec_score` & `matthews_corrcoef`: As expected, \"No\" noise performs best, while \"Both\" worst, in terms of whole avg.\n",
    "2. Interestingly, \"Bug\" i.e. (So recall=1.0) always performs better than \"Non-Bug\" i.e. (So precision=1.0)\n",
    "3. Avg Rank: pr_rec_score- `[1.74, 1.95, 2.82, 3.48]`, Mathew- `[1.78 , 2.46, 2.59, 3.17]`   (['No','Bug','Non-Bug','Both'] in that serial)\n",
    "4. So for Mathew, in terms of Avg Rank, Non-bug is better, contradicting point 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd,os\n",
    "from scipy.stats import wilcoxon, friedmanchisquare, rankdata, trim_mean, linregress\n",
    "import scikit_posthocs as sp\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PN = pd.read_csv(\"PN10.csv\",header=[0,1,2,3],index_col=0)\n",
    "NP = pd.read_csv(\"NP10.csv\",header=[0,1,2,3],index_col=0)\n",
    "clean = pd.read_csv(\"Results/Clean.csv\",header=[0,1,2],index_col=0)\n",
    "clean = clean.drop(columns=['UBag'],level=0)\n",
    "clean = clean.loc[PN.index]\n",
    "clean.shape,PN.shape,NP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_DROP = ['matthews_corrcoef']\n",
    "PN = PN.drop(columns=METRIC_DROP,level=3,axis=1).droplevel(3,axis=1)  #pr_rec_score, matthews_corrcoef\n",
    "NP = NP.drop(columns=METRIC_DROP,level=3,axis=1).droplevel(3,axis=1)\n",
    "clean = clean.drop(columns=METRIC_DROP,level=2,axis=1).droplevel(2,axis=1)\n",
    "NP.shape,PN.shape, clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(clean.columns.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRACS = [0.,.25,.5,.75,1.0]\n",
    "pn,np = {},{}\n",
    "for c in FRACS:\n",
    "    cpn = PN[str(c)] - clean \n",
    "    cnp = NP[str(c)] - clean\n",
    "    pn[c] = trim_mean(cpn.values.reshape(-1),.05)\n",
    "    np[c] = trim_mean(cnp.values.reshape(-1),.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(np.keys())\n",
    "metr = 'dAPRC'\n",
    "np_df = pd.DataFrame({'Noise Level':X,metr:[np[c] for c in X]})\n",
    "pn_df = pd.DataFrame({'Noise Level':X,metr:[pn[c] for c in X]})\n",
    "plt.xticks(FRACS)\n",
    "sns.regplot(x='Noise Level',y=metr,data=np_df,ci=95,scatter=True,label='NP')\n",
    "fig = sns.regplot(x='Noise Level',y=metr,data=pn_df,ci=95,scatter=True,label='PN')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form = 'pdf'\n",
    "fig.get_figure().savefig(f'figures/PN_NP_{metr}.{form}',format=form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meth = 'spearman'\n",
    "pg.corr(X,list(np.values()),method=meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.corr(X,list(pn.values()),method=meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linregress(X,list(pn.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linregress(X,list(np.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to Answer\n",
    "1. Does Filtering on noise estimates worsen imbalance? Before vs after IR for CLNI, Two-stage ensemble\n",
    "2. Avg. noise likelihood of minority vs majority class?\n",
    "3. Ratio of 0->1 & 1->0 noise types vs ratio of noisy samples identified in each class. With oracle access to no of actual, total no. of noisy samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shihab/anaconda3/envs/ana/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np,os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import \n",
    "from sklearn.metrics import matthews_corrcoef, precision_recall_curve, auc, accuracy_score, precision_score, recall_score\n",
    "import seaborn as sns,matplotlib.pyplot as plt\n",
    "\n",
    "from library.utils import evaluate, read_data\n",
    "from library.cleaners import kDN, ih_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "+ Using `kDN(K=5)`: In `25/32` datasets, no of samples with NP noise (Bug class in noisy dataset but actually Clean) has been overestimated. Consequently PN has been underestimated. This shows minority class gets disproportionately affected.\n",
    "    + For `ih_prob`: that number is `23/32`\n",
    "+ In terms of avg. probability of being noisy: samples in Bug class (of noisy data version) have vastly hugher value than clean ones. On further avg on 32 datasets, `.702 vs .074`, almost 10 times higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASETS = ['groovy-1_5_7.csv','jruby-1.4.0.csv','lucene-2.9.0.csv','jruby-1.7.0.preview1.csv','groovy-1_6_BETA_1.csv',\n",
    "        'derby-10.2.1.6.csv','wicket-1.5.3.csv','camel-2.9.0.csv','camel-1.4.0.csv','activemq-5.8.0.csv']\n",
    "DATASETS = [f for f in os.listdir(\"JIRA/\") if 'csv' in f]\n",
    "len(DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ih():\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    rf = RandomForestClassifier(n_estimators=50,n_jobs=4)\n",
    "\n",
    "    probabilities = np.zeros(Y.shape[0], dtype=float)\n",
    "    for train_index, test_index in skf.split(X,Y):\n",
    "        rf.fit(X[train_index], Y[train_index])\n",
    "        probs = rf.predict_proba(X[test_index])\n",
    "        probabilities[test_index] = probs[range(len(test_index)), Y[test_index]]\n",
    "\n",
    "    hardness = 1 - probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=DATASETS,columns=['PN','ePN','NP','eNP','bug-ih','clean-ih'])\n",
    "for d in DATASETS:\n",
    "    X,y_noisy,y_real = read_data(d,True)\n",
    "    PN = y_noisy[y_real==1]==0   #So, really 1(P), but now 0(N)\n",
    "    NP = y_noisy[y_real==0]==1   #So, really 0(N), but now 1(P)\n",
    "    assert PN.sum()+NP.sum()==(y_noisy!=y_real).sum()\n",
    "    print(f\"PN {PN.sum()},NP {NP.sum()}\")\n",
    "    ne = ih_prob(X,y_noisy,K=5)\n",
    "    \n",
    "    avg_bug,avg_clean = ne[y_noisy==1].mean(),ne[y_noisy==0].mean()  #avg_bug: avg noise probs of those labelled Buggy in Noisy dataset\n",
    "    actual_total = (y_noisy!=y_real).sum()\n",
    "    nidx = np.argsort(ne)[-actual_total:]\n",
    "    eNP = y_noisy[nidx]==1\n",
    "    ePN = y_noisy[nidx]==0\n",
    "    print(f\"ePN {ePN.sum()},eNP {eNP.sum()}\")\n",
    "    print(f\"Avg, Bug:{avg_bug:.3f}, Clean:{avg_clean:.3f}\")\n",
    "    print()\n",
    "    df.loc[d,:] = [x.sum() for x in [PN,ePN,NP,eNP]] + [avg_bug,avg_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.plot(range(len(DATASETS)),df['PN'],c='r',label='Actual');\n",
    "plt.plot(range(len(DATASETS)),df['ePN'],c='b',label='Predicted');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['ePN']>df['PN']).sum(),(df['eNP']>df['NP']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.plot(range(len(DATASETS)),df['NP'],c='r',label='Actual');\n",
    "plt.plot(range(len(DATASETS)),df['eNP'],c='b',label='Predicted');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.plot(range(len(DATASETS)),df['avg-bug'],c='r',label='Buggy');\n",
    "plt.plot(range(len(DATASETS)),df['avg-clean'],c='b',label='Clean');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg-bug'].mean(),df['avg-clean'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.boxplot(data=df[['avg-bug','avg-clean']]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ih = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdn = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = kdn[['avg-bug','avg-clean']].assign(detect='kDN')\n",
    "b = ih[['avg-bug','avg-clean']].assign(detect='IH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.concat([a,b],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.melt(tmp,id_vars=['detect'])\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.columns = ['detector','Class','MCC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5));\n",
    "f = sns.boxplot(x='Class',y='MCC',data=tt,hue='detector');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.get_figure().savefig(\"figures/noise_estimates.svg\",format='svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact on Classification performance\n",
    "So, both noise detectors discriminate against minority Bug class. But when we incorporate this noise estimates into this imbalanced classification setting, how much does this actually hurt classification performance? Is balancing first better? `Balancing->Filtering` vs `Filtering->Balancing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, RUSBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, TomekLinks\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from library.cleaners import FilteringEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateFilter(clf,X,y_noisy,y_real,cv,scorers):\n",
    "    scores = defaultdict(list)\n",
    "    to_keep = (y_noisy==y_real).sum()\n",
    "    for train_id, test_id in cv.split(X,y_real):  #vs y_noisy, to solve no-pos-label-in-test-set bug     \n",
    "        try:\n",
    "            clf = clf.fit(X[train_id],y_noisy[train_id],to_keep=to_keep)\n",
    "            probs = clf.predict_proba(X[test_id])\n",
    "            assert probs.shape[1]==2\n",
    "        except (ValueError,AssertionError) as e: \n",
    "            #assert str(e).startswith(\"Expected n_neighbors\") or str(e).startswith(\"The target\"),f\"Some different Error:{str(e)}\"\n",
    "            for func in scorers:\n",
    "                scores[func.__name__].append(0)\n",
    "            print(\"ERROR\")\n",
    "            continue\n",
    "        labels = np.argmax(probs,axis=1)\n",
    "        for func in scorers:\n",
    "            yp = probs[:,1]\n",
    "            try:\n",
    "                func([0,1,1],[.2,.6,.7])\n",
    "                yp = probs[:,1]\n",
    "            except ValueError as e:\n",
    "                yp = labels\n",
    "            scores[func.__name__].append(func(y_real[test_id],yp))\n",
    "    for func in scorers:\n",
    "        scores[func.__name__] = np.array(scores[func.__name__])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbs = {\n",
    "    'smote': SMOTE(k_neighbors=5),\n",
    "    'rus': RandomUnderSampler('not minority'),\n",
    "    'wilson':EditedNearestNeighbours(n_neighbors=5),  #Default was 3\n",
    "    'tomek': TomekLinks(),\n",
    "}\n",
    "clfs = {\n",
    "    'dt': DecisionTreeClassifier(max_depth=20),\n",
    "    'lr': LogisticRegression(solver='lbfgs',max_iter=1000),\n",
    "    'nb': GaussianNB(),\n",
    "    'knn': KNeighborsClassifier(n_neighbors=5),\n",
    "    'rf': RandomForestClassifier(n_estimators=50),\n",
    "}\n",
    "ensembles = {\n",
    "    'rboost_DT': RUSBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),algorithm='SAMME',n_estimators=10),\n",
    "    'rboost_NB': RUSBoostClassifier(base_estimator=GaussianNB(),algorithm='SAMME',n_estimators=10),\n",
    "    'bbag_DT': BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=20,max_features='sqrt')),\n",
    "    'bbag_NB': BalancedBaggingClassifier(base_estimator=GaussianNB()),\n",
    "}\n",
    "simples = {\n",
    "    'LR': LogisticRegression(solver='lbfgs',max_iter=1000),\n",
    "    'RF': RandomForestClassifier(n_estimators=50)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering -> Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for im,samp in imbs.items():\n",
    "    for c,clf in clfs.items():\n",
    "        models[(im,c)] = FilteringEstimator(Pipeline([('samp',samp),('clf',clf)]),kDN,K=5)\n",
    "\n",
    "for m,ens in ensembles.items():\n",
    "    models[('ens',m)] = FilteringEstimator(ens,kDN,K=5)\n",
    "    \n",
    "for m,clf in simples.items():\n",
    "    models[('sim',m)] = FilteringEstimator(clf,kDN,K=5)    \n",
    "    \n",
    "models.keys(),len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=None)\n",
    "def pr_rec_score(y,yp):\n",
    "    prec, rec, _ = precision_recall_curve(y,yp)\n",
    "    return auc(rec,prec)\n",
    "scorers = [matthews_corrcoef,pr_rec_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pd.MultiIndex.from_product([imbs.keys(),clfs.keys(),[f.__name__ for f in scorers]],names=['imb','clf','metric'])\n",
    "df = pd.DataFrame(index=DATASETS,columns=cols)\n",
    "df = pd.read_csv(\"Filtering->Balancing.csv\",header=[0,1,2],index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in DATASETS:\n",
    "    X,y_noisy,y_real = read_data(d,stats=True)\n",
    "    if df.loc[d,:].isna().sum()==0: \n",
    "        print(f\"Skipping {d}\")\n",
    "        continue\n",
    "    for k in models:\n",
    "        print(k)\n",
    "        r = evaluateFilter(models[k],X,y_noisy,y_real,cv,scorers)\n",
    "        for f in r:\n",
    "            df.loc[d,(k[0],k[1],f)] = r[f].mean()\n",
    "    df.to_csv(\"Filtering->Balancing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon, friedmanchisquare, rankdata\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = df.drop(columns=['matthews_corrcoef'],axis=1,level=2).droplevel(2,axis=1)\n",
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mat.copy()\n",
    "for d in mat.index:\n",
    "    res.loc[d] = rankdata(1-mat.loc[d])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
